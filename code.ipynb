{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VNL3abig3FtD",
        "colab_type": "code",
        "outputId": "818704bb-4b86-4078-cec9-087a25d56c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#link drive to google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UJZ5Z4Cx3ayF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "702beb75-604b-49a6-f747-ae5186565fc4"
      },
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WNcCP9kO3nRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "356aa6bc-7019-4523-e17d-90050dc722ea"
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from copy import copy\n",
        "\n",
        "#download from nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hx8fA4cfItsd",
        "colab_type": "code",
        "outputId": "9fb835b5-0a87-45b2-8f8f-17c87930cbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/guillaumegenthial/tf_metrics.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n",
            "  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-el3b8n9l\n",
            "Requirement already satisfied (use --upgrade to upgrade): tf-metrics==0.0.1 from git+https://github.com/guillaumegenthial/tf_metrics.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.14.6)\n",
            "Requirement already satisfied: tensorflow>=1.6 in /usr/local/lib/python3.6/dist-packages (from tf-metrics==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (0.32.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.0.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (1.0.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (3.6.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.6->tf-metrics==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.6->tf-metrics==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.6->tf-metrics==0.0.1) (3.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.6->tf-metrics==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.6->tf-metrics==0.0.1) (40.6.2)\n",
            "Building wheels for collected packages: tf-metrics\n",
            "  Running setup.py bdist_wheel for tf-metrics ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-kuxximcf/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n",
            "Successfully built tf-metrics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u1cBS1DqX_uq",
        "colab_type": "code",
        "outputId": "e465364c-6d0a-41e4-d592-9ca53646a426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/guillaumegenthial/tf_ner.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tf_ner'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/165)   \u001b[K\rremote: Counting objects:   1% (2/165)   \u001b[K\rremote: Counting objects:   2% (4/165)   \u001b[K\rremote: Counting objects:   3% (5/165)   \u001b[K\rremote: Counting objects:   4% (7/165)   \u001b[K\rremote: Counting objects:   5% (9/165)   \u001b[K\rremote: Counting objects:   6% (10/165)   \u001b[K\rremote: Counting objects:   7% (12/165)   \u001b[K\rremote: Counting objects:   8% (14/165)   \u001b[K\rremote: Counting objects:   9% (15/165)   \u001b[K\rremote: Counting objects:  10% (17/165)   \u001b[K\rremote: Counting objects:  11% (19/165)   \u001b[K\rremote: Counting objects:  12% (20/165)   \u001b[K\rremote: Counting objects:  13% (22/165)   \u001b[K\rremote: Counting objects:  14% (24/165)   \u001b[K\rremote: Counting objects:  15% (25/165)   \u001b[K\rremote: Counting objects:  16% (27/165)   \u001b[K\rremote: Counting objects:  17% (29/165)   \u001b[K\rremote: Counting objects:  18% (30/165)   \u001b[K\rremote: Counting objects:  19% (32/165)   \u001b[K\rremote: Counting objects:  20% (33/165)   \u001b[K\rremote: Counting objects:  21% (35/165)   \u001b[K\rremote: Counting objects:  22% (37/165)   \rremote: Counting objects:  23% (38/165)   \u001b[K\rremote: Counting objects:  24% (40/165)   \u001b[K\rremote: Counting objects:  25% (42/165)   \u001b[K\rremote: Counting objects:  26% (43/165)   \u001b[K\rremote: Counting objects:  27% (45/165)   \u001b[K\rremote: Counting objects:  28% (47/165)   \u001b[K\rremote: Counting objects:  29% (48/165)   \u001b[K\rremote: Counting objects:  30% (50/165)   \u001b[K\rremote: Counting objects:  31% (52/165)   \u001b[K\rremote: Counting objects:  32% (53/165)   \u001b[K\rremote: Counting objects:  33% (55/165)   \u001b[K\rremote: Counting objects:  34% (57/165)   \u001b[K\rremote: Counting objects:  35% (58/165)   \u001b[K\rremote: Counting objects:  36% (60/165)   \u001b[K\rremote: Counting objects:  37% (62/165)   \u001b[K\rremote: Counting objects:  38% (63/165)   \u001b[K\rremote: Counting objects:  39% (65/165)   \u001b[K\rremote: Counting objects:  40% (66/165)   \rremote: Counting objects:  41% (68/165)   \u001b[K\rremote: Counting objects:  42% (70/165)   \u001b[K\rremote: Counting objects:  43% (71/165)   \u001b[K\rremote: Counting objects:  44% (73/165)   \u001b[K\rremote: Counting objects:  45% (75/165)   \u001b[K\rremote: Counting objects:  46% (76/165)   \u001b[K\rremote: Counting objects:  47% (78/165)   \u001b[K\rremote: Counting objects:  48% (80/165)   \u001b[K\rremote: Counting objects:  49% (81/165)   \u001b[K\rremote: Counting objects:  50% (83/165)   \u001b[K\rremote: Counting objects:  51% (85/165)   \u001b[K\rremote: Counting objects:  52% (86/165)   \u001b[K\rremote: Counting objects:  53% (88/165)   \u001b[K\rremote: Counting objects:  54% (90/165)   \u001b[K\rremote: Counting objects:  55% (91/165)   \u001b[K\rremote: Counting objects:  56% (93/165)   \u001b[K\rremote: Counting objects:  57% (95/165)   \u001b[K\rremote: Counting objects:  58% (96/165)   \u001b[K\rremote: Counting objects:  59% (98/165)   \u001b[K\rremote: Counting objects:  60% (99/165)   \u001b[K\rremote: Counting objects:  61% (101/165)   \u001b[K\rremote: Counting objects:  62% (103/165)   \u001b[K\rremote: Counting objects:  63% (104/165)   \u001b[K\rremote: Counting objects:  64% (106/165)   \u001b[K\rremote: Counting objects:  65% (108/165)   \u001b[K\rremote: Counting objects:  66% (109/165)   \u001b[K\rremote: Counting objects:  67% (111/165)   \u001b[K\rremote: Counting objects:  68% (113/165)   \u001b[K\rremote: Counting objects:  69% (114/165)   \u001b[K\rremote: Counting objects:  70% (116/165)   \u001b[K\rremote: Counting objects:  71% (118/165)   \u001b[K\rremote: Counting objects:  72% (119/165)   \u001b[K\rremote: Counting objects:  73% (121/165)   \u001b[K\rremote: Counting objects:  74% (123/165)   \u001b[K\rremote: Counting objects:  75% (124/165)   \u001b[K\rremote: Counting objects:  76% (126/165)   \u001b[K\rremote: Counting objects:  77% (128/165)   \u001b[K\rremote: Counting objects:  78% (129/165)   \u001b[K\rremote: Counting objects:  79% (131/165)   \u001b[K\rremote: Counting objects:  80% (132/165)   \u001b[K\rremote: Counting objects:  81% (134/165)   \u001b[K\rremote: Counting objects:  82% (136/165)   \u001b[K\rremote: Counting objects:  83% (137/165)   \u001b[K\rremote: Counting objects:  84% (139/165)   \u001b[K\rremote: Counting objects:  85% (141/165)   \u001b[K\rremote: Counting objects:  86% (142/165)   \u001b[K\rremote: Counting objects:  87% (144/165)   \u001b[K\rremote: Counting objects:  88% (146/165)   \u001b[K\rremote: Counting objects:  89% (147/165)   \u001b[K\rremote: Counting objects:  90% (149/165)   \u001b[K\rremote: Counting objects:  91% (151/165)   \u001b[K\rremote: Counting objects:  92% (152/165)   \u001b[K\rremote: Counting objects:  93% (154/165)   \u001b[K\rremote: Counting objects:  94% (156/165)   \u001b[K\rremote: Counting objects:  95% (157/165)   \u001b[K\rremote: Counting objects:  96% (159/165)   \u001b[K\rremote: Counting objects:  97% (161/165)   \u001b[K\rremote: Counting objects:  98% (162/165)   \u001b[K\rremote: Counting objects:  99% (164/165)   \u001b[K\rremote: Counting objects: 100% (165/165)   \u001b[K\rremote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/103)   \u001b[K\rremote: Compressing objects:   1% (2/103)   \u001b[K\rremote: Compressing objects:   2% (3/103)   \u001b[K\rremote: Compressing objects:   3% (4/103)   \u001b[K\rremote: Compressing objects:   4% (5/103)   \u001b[K\rremote: Compressing objects:   5% (6/103)   \u001b[K\rremote: Compressing objects:   6% (7/103)   \u001b[K\rremote: Compressing objects:   7% (8/103)   \u001b[K\rremote: Compressing objects:   8% (9/103)   \u001b[K\rremote: Compressing objects:   9% (10/103)   \u001b[K\rremote: Compressing objects:  10% (11/103)   \u001b[K\rremote: Compressing objects:  11% (12/103)   \u001b[K\rremote: Compressing objects:  12% (13/103)   \u001b[K\rremote: Compressing objects:  13% (14/103)   \u001b[K\rremote: Compressing objects:  14% (15/103)   \u001b[K\rremote: Compressing objects:  15% (16/103)   \u001b[K\rremote: Compressing objects:  16% (17/103)   \u001b[K\rremote: Compressing objects:  17% (18/103)   \u001b[K\rremote: Compressing objects:  18% (19/103)   \u001b[K\rremote: Compressing objects:  19% (20/103)   \u001b[K\rremote: Compressing objects:  20% (21/103)   \u001b[K\rremote: Compressing objects:  21% (22/103)   \u001b[K\rremote: Compressing objects:  22% (23/103)   \u001b[K\rremote: Compressing objects:  23% (24/103)   \u001b[K\rremote: Compressing objects:  24% (25/103)   \u001b[K\rremote: Compressing objects:  25% (26/103)   \u001b[K\rremote: Compressing objects:  26% (27/103)   \u001b[K\rremote: Compressing objects:  27% (28/103)   \u001b[K\rremote: Compressing objects:  28% (29/103)   \u001b[K\rremote: Compressing objects:  29% (30/103)   \u001b[K\rremote: Compressing objects:  30% (31/103)   \u001b[K\rremote: Compressing objects:  31% (32/103)   \u001b[K\rremote: Compressing objects:  32% (33/103)   \u001b[K\rremote: Compressing objects:  33% (34/103)   \u001b[K\rremote: Compressing objects:  34% (36/103)   \u001b[K\rremote: Compressing objects:  35% (37/103)   \u001b[K\rremote: Compressing objects:  36% (38/103)   \u001b[K\rremote: Compressing objects:  37% (39/103)   \u001b[K\rremote: Compressing objects:  38% (40/103)   \u001b[K\rremote: Compressing objects:  39% (41/103)   \u001b[K\rremote: Compressing objects:  40% (42/103)   \u001b[K\rremote: Compressing objects:  41% (43/103)   \u001b[K\rremote: Compressing objects:  42% (44/103)   \u001b[K\rremote: Compressing objects:  43% (45/103)   \u001b[K\rremote: Compressing objects:  44% (46/103)   \u001b[K\rremote: Compressing objects:  45% (47/103)   \u001b[K\rremote: Compressing objects:  46% (48/103)   \u001b[K\rremote: Compressing objects:  47% (49/103)   \u001b[K\rremote: Compressing objects:  48% (50/103)   \u001b[K\rremote: Compressing objects:  49% (51/103)   \u001b[K\rremote: Compressing objects:  50% (52/103)   \u001b[K\rremote: Compressing objects:  51% (53/103)   \u001b[K\rremote: Compressing objects:  52% (54/103)   \u001b[K\rremote: Compressing objects:  53% (55/103)   \u001b[K\rremote: Compressing objects:  54% (56/103)   \u001b[K\rremote: Compressing objects:  55% (57/103)   \u001b[K\rremote: Compressing objects:  56% (58/103)   \u001b[K\rremote: Compressing objects:  57% (59/103)   \u001b[K\rremote: Compressing objects:  58% (60/103)   \u001b[K\rremote: Compressing objects:  59% (61/103)   \u001b[K\rremote: Compressing objects:  60% (62/103)   \u001b[K\rremote: Compressing objects:  61% (63/103)   \u001b[K\rremote: Compressing objects:  62% (64/103)   \u001b[K\rremote: Compressing objects:  63% (65/103)   \u001b[K\rremote: Compressing objects:  64% (66/103)   \u001b[K\rremote: Compressing objects:  65% (67/103)   \u001b[K\rremote: Compressing objects:  66% (68/103)   \u001b[K\rremote: Compressing objects:  67% (70/103)   \u001b[K\rremote: Compressing objects:  68% (71/103)   \u001b[K\rremote: Compressing objects:  69% (72/103)   \u001b[K\rremote: Compressing objects:  70% (73/103)   \u001b[K\rremote: Compressing objects:  71% (74/103)   \u001b[K\rremote: Compressing objects:  72% (75/103)   \u001b[K\rremote: Compressing objects:  73% (76/103)   \u001b[K\rremote: Compressing objects:  74% (77/103)   \u001b[K\rremote: Compressing objects:  75% (78/103)   \u001b[K\rremote: Compressing objects:  76% (79/103)   \u001b[K\rremote: Compressing objects:  77% (80/103)   \u001b[K\rremote: Compressing objects:  78% (81/103)   \u001b[K\rremote: Compressing objects:  79% (82/103)   \u001b[K\rremote: Compressing objects:  80% (83/103)   \u001b[K\rremote: Compressing objects:  81% (84/103)   \u001b[K\rremote: Compressing objects:  82% (85/103)   \u001b[K\rremote: Compressing objects:  83% (86/103)   \u001b[K\rremote: Compressing objects:  84% (87/103)   \u001b[K\rremote: Compressing objects:  85% (88/103)   \u001b[K\rremote: Compressing objects:  86% (89/103)   \u001b[K\rremote: Compressing objects:  87% (90/103)   \u001b[K\rremote: Compressing objects:  88% (91/103)   \u001b[K\rremote: Compressing objects:  89% (92/103)   \u001b[K\rremote: Compressing objects:  90% (93/103)   \u001b[K\rremote: Compressing objects:  91% (94/103)   \u001b[K\rremote: Compressing objects:  92% (95/103)   \u001b[K\rremote: Compressing objects:  93% (96/103)   \u001b[K\rremote: Compressing objects:  94% (97/103)   \u001b[K\rremote: Compressing objects:  95% (98/103)   \u001b[K\rremote: Compressing objects:  96% (99/103)   \u001b[K\rremote: Compressing objects:  97% (100/103)   \u001b[K\rremote: Compressing objects:  98% (101/103)   \u001b[K\rremote: Compressing objects:  99% (102/103)   \u001b[K\rremote: Compressing objects: 100% (103/103)   \u001b[K\rremote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "Receiving objects:   0% (1/165)   \rReceiving objects:   1% (2/165)   \rReceiving objects:   2% (4/165)   \rReceiving objects:   3% (5/165)   \rReceiving objects:   4% (7/165)   \rReceiving objects:   5% (9/165)   \rReceiving objects:   6% (10/165)   \rReceiving objects:   7% (12/165)   \rReceiving objects:   8% (14/165)   \rReceiving objects:   9% (15/165)   \rReceiving objects:  10% (17/165)   \rReceiving objects:  11% (19/165)   \rReceiving objects:  12% (20/165)   \rReceiving objects:  13% (22/165)   \rReceiving objects:  14% (24/165)   \rReceiving objects:  15% (25/165)   \rReceiving objects:  16% (27/165)   \rReceiving objects:  17% (29/165)   \rReceiving objects:  18% (30/165)   \rReceiving objects:  19% (32/165)   \rReceiving objects:  20% (33/165)   \rReceiving objects:  21% (35/165)   \rReceiving objects:  22% (37/165)   \rReceiving objects:  23% (38/165)   \rReceiving objects:  24% (40/165)   \rReceiving objects:  25% (42/165)   \rReceiving objects:  26% (43/165)   \rReceiving objects:  27% (45/165)   \rReceiving objects:  28% (47/165)   \rReceiving objects:  29% (48/165)   \rReceiving objects:  30% (50/165)   \rReceiving objects:  31% (52/165)   \rReceiving objects:  32% (53/165)   \rReceiving objects:  33% (55/165)   \rReceiving objects:  34% (57/165)   \rReceiving objects:  35% (58/165)   \rReceiving objects:  36% (60/165)   \rReceiving objects:  37% (62/165)   \rReceiving objects:  38% (63/165)   \rReceiving objects:  39% (65/165)   \rReceiving objects:  40% (66/165)   \rReceiving objects:  41% (68/165)   \rReceiving objects:  42% (70/165)   \rReceiving objects:  43% (71/165)   \rReceiving objects:  44% (73/165)   \rReceiving objects:  45% (75/165)   \rReceiving objects:  46% (76/165)   \rReceiving objects:  47% (78/165)   \rReceiving objects:  48% (80/165)   \rReceiving objects:  49% (81/165)   \rReceiving objects:  50% (83/165)   \rReceiving objects:  51% (85/165)   \rReceiving objects:  52% (86/165)   \rReceiving objects:  53% (88/165)   \rReceiving objects:  54% (90/165)   \rReceiving objects:  55% (91/165)   \rReceiving objects:  56% (93/165)   \rReceiving objects:  57% (95/165)   \rReceiving objects:  58% (96/165)   \rReceiving objects:  59% (98/165)   \rReceiving objects:  60% (99/165)   \rReceiving objects:  61% (101/165)   \rReceiving objects:  62% (103/165)   \rReceiving objects:  63% (104/165)   \rReceiving objects:  64% (106/165)   \rReceiving objects:  65% (108/165)   \rReceiving objects:  66% (109/165)   \rReceiving objects:  67% (111/165)   \rReceiving objects:  68% (113/165)   \rReceiving objects:  69% (114/165)   \rReceiving objects:  70% (116/165)   \rReceiving objects:  71% (118/165)   \rReceiving objects:  72% (119/165)   \rReceiving objects:  73% (121/165)   \rReceiving objects:  74% (123/165)   \rReceiving objects:  75% (124/165)   \rremote: Total 165 (delta 70), reused 143 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects:  76% (126/165)   \rReceiving objects:  77% (128/165)   \rReceiving objects:  78% (129/165)   \rReceiving objects:  79% (131/165)   \rReceiving objects:  80% (132/165)   \rReceiving objects:  81% (134/165)   \rReceiving objects:  82% (136/165)   \rReceiving objects:  83% (137/165)   \rReceiving objects:  84% (139/165)   \rReceiving objects:  85% (141/165)   \rReceiving objects:  86% (142/165)   \rReceiving objects:  87% (144/165)   \rReceiving objects:  88% (146/165)   \rReceiving objects:  89% (147/165)   \rReceiving objects:  90% (149/165)   \rReceiving objects:  91% (151/165)   \rReceiving objects:  92% (152/165)   \rReceiving objects:  93% (154/165)   \rReceiving objects:  94% (156/165)   \rReceiving objects:  95% (157/165)   \rReceiving objects:  96% (159/165)   \rReceiving objects:  97% (161/165)   \rReceiving objects:  98% (162/165)   \rReceiving objects:  99% (164/165)   \rReceiving objects: 100% (165/165)   \rReceiving objects: 100% (165/165), 126.19 KiB | 3.00 MiB/s, done.\n",
            "Resolving deltas:   0% (0/70)   \rResolving deltas:  11% (8/70)   \rResolving deltas:  15% (11/70)   \rResolving deltas:  20% (14/70)   \rResolving deltas:  22% (16/70)   \rResolving deltas:  24% (17/70)   \rResolving deltas:  35% (25/70)   \rResolving deltas:  38% (27/70)   \rResolving deltas:  45% (32/70)   \rResolving deltas:  47% (33/70)   \rResolving deltas:  48% (34/70)   \rResolving deltas:  50% (35/70)   \rResolving deltas:  52% (37/70)   \rResolving deltas:  54% (38/70)   \rResolving deltas:  57% (40/70)   \rResolving deltas:  62% (44/70)   \rResolving deltas:  64% (45/70)   \rResolving deltas:  65% (46/70)   \rResolving deltas:  68% (48/70)   \rResolving deltas:  70% (49/70)   \rResolving deltas: 100% (70/70)   \rResolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4gCyhma2rvoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Ayurveda PDF Files**"
      ]
    },
    {
      "metadata": {
        "id": "UQrgO0WS5Kxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read text file for train data and create data file in which to write processed data\n",
        "fread = open('test_data.txt')  \n",
        "fwrite = open('test_paras.txt','w+')\n",
        "lines = fread.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "04BIlAh2R7GA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#each para is one document\n",
        "paras=[]\n",
        "para=\"\"\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line=='':\n",
        "    paras.append(para) \n",
        "    para=\"\"\n",
        "  else:\n",
        "    para=para+\" \"+line.strip()\n",
        "paras.append(para)\n",
        "print(len(paras))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W_zIJ6X-Gysz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#finish preprocessing\n",
        "count=1\n",
        "for line in paras:\n",
        "    line = line.strip()\n",
        "    line = line.lower()\n",
        "    \n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    tmp = tokens[:]\n",
        "    #so w/w is removed. but problem with removal of brackets\n",
        "    for token in tokens:\n",
        "      if token in ['(','[',']',')']:\n",
        "        continue\n",
        "      elif not token.isalnum(): \n",
        "        tmp.remove(token)\n",
        "    for n,t in enumerate(tokens):\n",
        "      if t in ['(','['] and t in tmp:\n",
        "        tmp.remove(t)\n",
        "        for f in tokens[n:]:\n",
        "          if f in [')',']'] and f in tmp:\n",
        "            tmp.remove(f)\n",
        "            break\n",
        "          if f in tmp:\n",
        "            tmp.remove(f)\n",
        "      elif len(t)<=2 and t in tmp  and t not in stop:\n",
        "        tmp.remove(t)  \n",
        "\n",
        "    string = \" \".join(tmp)\n",
        "    words = re.findall(r'\\b[A-Za-z]+\\b',string)\n",
        "\n",
        "    for word in words:\n",
        "      fwrite.write(word+' ')\n",
        "    fwrite.write('\\n\\n')\n",
        "\n",
        "fwrite.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-IuTw60sHjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Create Own Word Embeddings**"
      ]
    },
    {
      "metadata": {
        "id": "OdhsTlEqQexq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and load pre trained glove embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Ie8kITBGRo8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7b798c3-4095-4151-c0a6-b4701b51cd2b"
      },
      "cell_type": "code",
      "source": [
        "cd tf_ner/data/example/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tf_ner/data/example\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DzLMUmxuQnho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "97ce6c0a-6b08-45f3-f066-6000ebec5b28"
      },
      "cell_type": "code",
      "source": [
        "!make download-glove\n",
        "!make build"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wget -P . \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
            "--2018-12-04 08:22:34--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2018-12-04 08:22:34--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘./glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  27.9MB/s    in 53s     \n",
            "\n",
            "2018-12-04 08:23:26 (39.4 MB/s) - ‘./glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "unzip glove.840B.300d.zip -d glove.840B.300d.txt\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt/glove.840B.300d.txt  \n",
            "rm glove.840B.300d.zip\n",
            "python build_vocab.py\n",
            "Build vocab words (may take a while)\n",
            "- done. Kept 22 out of 22\n",
            "Build vocab chars\n",
            "- done. Found 32 chars\n",
            "Build vocab tags (may take a while)\n",
            "- done. Found 4 tags.\n",
            "python build_glove.py\n",
            "Reading GloVe file (may take a while)\n",
            "Traceback (most recent call last):\n",
            "  File \"build_glove.py\", line 27, in <module>\n",
            "    with Path('glove.840B.300d.txt').open() as f:\n",
            "  File \"/usr/lib/python3.6/pathlib.py\", line 1183, in open\n",
            "    opener=self._opener)\n",
            "IsADirectoryError: [Errno 21] Is a directory: 'glove.840B.300d.txt'\n",
            "Makefile:7: recipe for target 'build' failed\n",
            "make: *** [build] Error 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7HpEzMMaptHb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Fine tune glove embeddings with domain specifiic data"
      ]
    },
    {
      "metadata": {
        "id": "eWDpfRLpsOuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename) as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfS6yNmWs43B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "original_embedding = glove2dict(\"/content/tf_ner/data/example/glove.840B.300d.txt/glove.840B.300d.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRaIEjW-tAhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bb17fbc-65ed-4b12-f5d1-11091fac63ae"
      },
      "cell_type": "code",
      "source": [
        "#mittens provides an interface for fine tuning word embeddings\n",
        "!pip3 install mittens\n",
        "from mittens import Mittens"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hDBwDtnPtk3N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testa = open('/content/drive/My Drive/NLP Project/testa.words.txt')\n",
        "testb = open('/content/drive/My Drive/NLP Project/testb.words.txt')\n",
        "train = open('/content/drive/My Drive/NLP Project/train/train.words.txt')\n",
        "linesa = testa.readlines()\n",
        "linesb = testb.readlines()\n",
        "lines = train.readlines()\n",
        "vocab_list = []\n",
        "vb_list = []\n",
        "for line in linesa:\n",
        "  tokens = word_tokenize(line)\n",
        "  vb_list.append(tokens)\n",
        "  vocab_list = vocab_list + tokens \n",
        "for line in linesb:\n",
        "  tokens = word_tokenize(line)\n",
        "  vb_list.append(tokens)\n",
        "  vocab_list = vocab_list + tokens \n",
        "for line in lines:\n",
        "  tokens = word_tokenize(line)\n",
        "  vb_list.append(tokens)\n",
        "  vocab_list = vocab_list + tokens \n",
        "myset = set(vocab_list)\n",
        "vocab = list(myset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4idhdwFbwCe7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "document = vb_list\n",
        "names = vocab\n",
        "array = [[0 for i in range(len(vocab))] for j in range(len(vocab))]\n",
        "\n",
        "for l in document:\n",
        "    for i in range(len(l)):\n",
        "        for item in l[:i] + l[i + 1:]:\n",
        "            array[i][vocab.index(item)] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bu5ASsg64IzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "array = np.array(array)\n",
        "cooccurrence = array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19TlTyjytHW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8641ce2c-407f-442f-ddec-8d834191de08"
      },
      "cell_type": "code",
      "source": [
        "mittens_model = Mittens(n = 300, max_iter=1000)\n",
        "new_embeddings = mittens_model.fit(cooccurrence, vocab = vocab, initial_embedding_dict= original_embedding)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss: 86.0621337890625"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ioWDAQhRr3mq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Tensorflow - Named Entity Recognition**\n",
        "\n",
        "https://github.com/guillaumegenthial/tf_ner"
      ]
    },
    {
      "metadata": {
        "id": "e9Q-Cf6cdD3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5267c4d-63dc-44bc-a6c4-66059a00fc8a"
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/My\\ Drive/NLP\\ Project/train"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP Project/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_1LNV3kfaVvN",
        "colab_type": "code",
        "outputId": "61d8a233-5812-421a-a75b-3d702a5f5ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!cp /content/drive/My\\ Drive/NLP\\ Project/train/train.words.txt .\n",
        "!cp /content/drive/My\\ Drive/NLP\\ Project/train/train.tags.txt ."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/NLP Project/train/train.words.txt': No such file or directory\n",
            "cp: cannot stat '/content/drive/My Drive/NLP Project/train/train.tags.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "obt8xfYbExkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create vocab and initialise word embeddings."
      ]
    },
    {
      "metadata": {
        "id": "CgVUC9rxjl7I",
        "colab_type": "code",
        "outputId": "c392f033-1731-43a5-c6b5-910b3922fbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!python build_vocab.py"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build vocab words (may take a while)\n",
            "- done. Kept 3789 out of 3789\n",
            "Build vocab chars\n",
            "- done. Found 26 chars\n",
            "Build vocab tags (may take a while)\n",
            "- done. Found 13 tags.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jrrTtnVdkwco",
        "colab_type": "code",
        "outputId": "78b4306b-3f16-48d9-d786-969f80de06f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with Path('vocab.words.txt').open() as f:\n",
        "  word_to_idx = {line.strip(): idx for idx, line in enumerate(f)}\n",
        "  size_vocab = len(word_to_idx)\n",
        "\n",
        "  # Array of zeros\n",
        "embeddings = np.zeros((size_vocab, 300))\n",
        "found = 0\n",
        "for n,v in enumerate(vocab):\n",
        "  word = v\n",
        "  embedding = new_embeddings[n]\n",
        "  if word in word_to_idx:\n",
        "      found += 1\n",
        "      word_idx = word_to_idx[word]\n",
        "      embeddings[word_idx] = embedding\n",
        "print('- done. Found {} vectors for {} words'.format(found, size_vocab))\n",
        "np.savez_compressed('/content/drive/My Drive/NLP Project/train/glove.npz', embeddings=embeddings)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- done. Found 3789 vectors for 3789 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xf0kuwwgFQyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Move to the directory where your model is present"
      ]
    },
    {
      "metadata": {
        "id": "m3vmwMdFpBMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/tf_ner/models/chars_conv_lstm_crf_ema/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJ0OIKePFWfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "90DvSZEcid7s",
        "colab_type": "code",
        "outputId": "657905ec-aca5-4650-cf39-fe1709017420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1941
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"GloVe Embeddings + chars conv and max pooling + bi-LSTM + CRF\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tf_metrics import precision, recall, f1\n",
        "\n",
        "from masked_conv import masked_conv1d_and_max\n",
        "\n",
        "DATADIR = '/content/drive/My Drive/NLP Project/train/'\n",
        "\n",
        "# Logging\n",
        "Path('results').mkdir(exist_ok=True)\n",
        "tf.logging.set_verbosity(logging.INFO)\n",
        "handlers = [\n",
        "    logging.FileHandler('results/main.log'),\n",
        "    logging.StreamHandler(sys.stdout)\n",
        "]\n",
        "logging.getLogger('tensorflow').handlers = handlers\n",
        "\n",
        "\n",
        "def parse_fn(line_words, line_tags):\n",
        "    # Encode in Bytes for TF\n",
        "    words = [w.encode() for w in line_words.strip().split()]\n",
        "    tags = [t.encode() for t in line_tags.strip().split()]\n",
        "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
        "\n",
        "    # Chars\n",
        "    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n",
        "    lengths = [len(c) for c in chars]\n",
        "    max_len = max(lengths)\n",
        "    chars = [c + [b'<pad>'] * (max_len - l) for c, l in zip(chars, lengths)]\n",
        "    return ((words, len(words)), (chars, lengths)), tags\n",
        "\n",
        "\n",
        "def generator_fn(words, tags):\n",
        "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
        "        for line_words, line_tags in zip(f_words, f_tags):\n",
        "            yield parse_fn(line_words, line_tags)\n",
        "\n",
        "\n",
        "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
        "    params = params if params is not None else {}\n",
        "    shapes = ((([None], ()),               # (words, nwords)\n",
        "               ([None, None], [None])),    # (chars, nchars)\n",
        "              [None])                      # tags\n",
        "    types = (((tf.string, tf.int32),\n",
        "              (tf.string, tf.int32)),\n",
        "             tf.string)\n",
        "    defaults = ((('<pad>', 0),\n",
        "                 ('<pad>', 0)),\n",
        "                'O')\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        functools.partial(generator_fn, words, tags),\n",
        "        output_shapes=shapes, output_types=types)\n",
        "\n",
        "    if shuffle_and_repeat:\n",
        "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
        "\n",
        "    dataset = (dataset\n",
        "               .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
        "               .prefetch(1))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def graph_fn(features, labels, mode, params, reuse=None, getter=None):\n",
        "    # Read vocabs and inputs\n",
        "    num_tags = params['num_tags']\n",
        "    (words, nwords), (chars, nchars) = features\n",
        "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    with tf.variable_scope('graph', reuse=reuse, custom_getter=getter):\n",
        "        # Read vocabs and inputs\n",
        "        dropout = params['dropout']\n",
        "        (words, nwords), (chars, nchars) = features\n",
        "        training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "        vocab_words = tf.contrib.lookup.index_table_from_file(\n",
        "            params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
        "        vocab_chars = tf.contrib.lookup.index_table_from_file(\n",
        "            params['chars'], num_oov_buckets=params['num_oov_buckets'])\n",
        "        with Path(params['tags']).open() as f:\n",
        "            indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
        "            num_tags = len(indices) + 1\n",
        "        with Path(params['chars']).open() as f:\n",
        "            num_chars = sum(1 for _ in f) + params['num_oov_buckets']\n",
        "\n",
        "        # Char Embeddings\n",
        "        char_ids = vocab_chars.lookup(chars)\n",
        "        variable = tf.get_variable(\n",
        "            'chars_embeddings', [num_chars + 1, params['dim_chars']], tf.float32)\n",
        "        char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n",
        "        char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n",
        "                                            training=training)\n",
        "\n",
        "        # Char LSTM\n",
        "        weights = tf.sequence_mask(nchars)\n",
        "        char_embeddings = masked_conv1d_and_max(\n",
        "            char_embeddings, weights, params['filters'], params['kernel_size'])\n",
        "\n",
        "        # Word Embeddings\n",
        "        word_ids = vocab_words.lookup(words)\n",
        "        glove = np.load(params['glove'])['embeddings']  # np.array\n",
        "        variable = np.vstack([glove, [[0.] * params['dim']]])\n",
        "        variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
        "        word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
        "\n",
        "        # Concatenate Word and Char Embeddings\n",
        "        embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n",
        "        embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
        "\n",
        "        # LSTM\n",
        "        t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n",
        "        lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "        lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "        lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
        "        output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "        output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "        output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "        output = tf.transpose(output, perm=[1, 0, 2])\n",
        "        output = tf.layers.dropout(output, rate=dropout, training=training)\n",
        "\n",
        "        # CRF\n",
        "        logits = tf.layers.dense(output, num_tags)\n",
        "        crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
        "        return logits, crf_params\n",
        "\n",
        "\n",
        "def ema_getter(ema):\n",
        "\n",
        "    def _ema_getter(getter, name, *args, **kwargs):\n",
        "        var = getter(name, *args, **kwargs)\n",
        "        ema_var = ema.average(var)\n",
        "        return ema_var if ema_var else var\n",
        "\n",
        "    return _ema_getter\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # For serving features are a bit different\n",
        "    if isinstance(features, dict):\n",
        "        features = ((features['words'], features['nwords']),\n",
        "                    (features['chars'], features['nchars']))\n",
        "\n",
        "    with Path(params['tags']).open() as f:\n",
        "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
        "        num_tags = len(indices) + 1\n",
        "        params['num_tags'] = num_tags\n",
        "\n",
        "    # Graph\n",
        "    (words, nwords), (chars, nchars) = features\n",
        "    logits, crf_params = graph_fn(features, labels, mode, params)\n",
        "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
        "\n",
        "    # Moving Average\n",
        "    variables = tf.get_collection('trainable_variables', 'graph')\n",
        "    ema = tf.train.ExponentialMovingAverage(0.999)\n",
        "    ema_op = ema.apply(variables)\n",
        "    logits_ema, crf_params_ema = graph_fn(\n",
        "        features, labels, mode, params, reuse=True, getter=ema_getter(ema))\n",
        "    pred_ids_ema, _ = tf.contrib.crf.crf_decode(\n",
        "        logits_ema, crf_params_ema, nwords)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Predictions\n",
        "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
        "            params['tags'])\n",
        "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
        "        pred_strings_ema = reverse_vocab_tags.lookup(tf.to_int64(pred_ids_ema))\n",
        "        predictions = {\n",
        "            'pred_ids': pred_ids,\n",
        "            'tags': pred_strings,\n",
        "            'pred_ids_ema': pred_ids_ema,\n",
        "            'tags_ema': pred_strings_ema,\n",
        "        }\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "    else:\n",
        "        # Loss\n",
        "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
        "        tags = vocab_tags.lookup(labels)\n",
        "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
        "            logits, tags, nwords, crf_params)\n",
        "        loss = tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "        # Metrics\n",
        "        weights = tf.sequence_mask(nwords)\n",
        "        metrics = {\n",
        "            'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
        "            'acc_ema': tf.metrics.accuracy(tags, pred_ids_ema, weights),\n",
        "            'pr': precision(tags, pred_ids, num_tags, indices, weights),\n",
        "            'pr_ema': precision(tags, pred_ids_ema, num_tags, indices, weights),\n",
        "            'rc': recall(tags, pred_ids, num_tags, indices, weights),\n",
        "            'rc_ema': recall(tags, pred_ids_ema, num_tags, indices, weights),\n",
        "            'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
        "            'f1_ema': f1(tags, pred_ids_ema, num_tags, indices, weights),\n",
        "        }\n",
        "        for metric_name, op in metrics.items():\n",
        "            tf.summary.scalar(metric_name, op[1])\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.EVAL:\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            train_op = tf.train.AdamOptimizer().minimize(\n",
        "                loss, global_step=tf.train.get_or_create_global_step(),\n",
        "                var_list=variables)\n",
        "            train_op = tf.group([train_op, ema_op])\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, train_op=train_op)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Params\n",
        "    params = {\n",
        "        'dim_chars': 100,\n",
        "        'dim': 300,\n",
        "        'dropout': 0.5,\n",
        "        'num_oov_buckets': 1,\n",
        "        'epochs': 25,\n",
        "        'batch_size': 5,\n",
        "        'buffer': 15000,\n",
        "        'filters': 50,\n",
        "        'kernel_size': 3,\n",
        "        'lstm_size': 100,\n",
        "        'words': str(Path(DATADIR, 'vocab.words.txt')),\n",
        "        'chars': str(Path(DATADIR, 'vocab.chars.txt')),\n",
        "        'tags': str(Path(DATADIR, 'vocab.tags.txt')),\n",
        "        'glove': str(Path(DATADIR, 'glove.npz'))\n",
        "    }\n",
        "    with Path('results/params.json').open('w') as f:\n",
        "        json.dump(params, f, indent=4, sort_keys=True)\n",
        "\n",
        "    def fwords(name):\n",
        "        return str(Path(DATADIR, '{}.words.txt'.format(name)))\n",
        "\n",
        "    def ftags(name):\n",
        "        return str(Path(DATADIR, '{}.tags.txt'.format(name)))\n",
        "\n",
        "    # Estimator, train and evaluate\n",
        "    train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
        "                                   params, shuffle_and_repeat=True)\n",
        "    eval_inpf = functools.partial(input_fn, fwords('testa'), ftags('testa'))\n",
        "\n",
        "    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
        "    estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
        "    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n",
        "    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
        "        estimator, 'f1_ema', 500, min_steps=8000, run_every_secs=120)\n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
        "    print(\"SSUP22\",train_spec,eval_spec)\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "    print(\"OH\")\n",
        "    # Write predictions to file\n",
        "    def write_predictions(name, mode):\n",
        "        \"\"\"Write predictions of dataset with name to file\"\"\"\n",
        "        Path('results/score').mkdir(parents=True, exist_ok=True)\n",
        "        with Path('results/score/{}.{}.preds.txt'.format(name, mode)).open('wb') as f:\n",
        "            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n",
        "            golds_gen = generator_fn(fwords(name), ftags(name))\n",
        "            preds_gen = estimator.predict(test_inpf)\n",
        "            for golds, preds in zip(golds_gen, preds_gen):\n",
        "                ((words, _), (_, _)), tags = golds\n",
        "                for word, tag, tag_pred in zip(words, tags, preds[mode]):\n",
        "                    f.write(b' '.join([word, tag, tag_pred]) + b'\\n')\n",
        "                f.write(b'\\n')\n",
        "\n",
        "    for name in ['train', 'testa', 'testb']:\n",
        "        for mode in ['tags', 'tags_ema']:\n",
        "          write_predictions(name, mode)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config: {'_model_dir': 'results/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 120, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26d3bbbf98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "SSUP22 TrainSpec(input_fn=functools.partial(<function input_fn at 0x7f26d3b71598>, '/content/drive/My Drive/NLP Project/train/train.words.txt', '/content/drive/My Drive/NLP Project/train/train.tags.txt', {'dim_chars': 100, 'dim': 300, 'dropout': 0.5, 'num_oov_buckets': 1, 'epochs': 25, 'batch_size': 5, 'buffer': 15000, 'filters': 50, 'kernel_size': 3, 'lstm_size': 100, 'words': '/content/drive/My Drive/NLP Project/train/vocab.words.txt', 'chars': '/content/drive/My Drive/NLP Project/train/vocab.chars.txt', 'tags': '/content/drive/My Drive/NLP Project/train/vocab.tags.txt', 'glove': '/content/drive/My Drive/NLP Project/train/glove.npz'}, shuffle_and_repeat=True), max_steps=None, hooks=(<tensorflow.contrib.estimator.python.estimator.early_stopping._StopOnPredicateHook object at 0x7f26d3bbb6d8>,)) EvalSpec(input_fn=functools.partial(<function input_fn at 0x7f26d3b71598>, '/content/drive/My Drive/NLP Project/train/testa.words.txt', '/content/drive/My Drive/NLP Project/train/testa.tags.txt'), steps=100, name=None, hooks=(), exporters=(), start_delay_secs=120, throttle_secs=120)\n",
            "Not using Distribute Coordinator.\n",
            "Running training and evaluation locally (non-distributed).\n",
            "Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 120.\n",
            "Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done calling model_fn.\n",
            "Create CheckpointSaverHook.\n",
            "Graph was finalized.\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Saving checkpoints for 0 into results/model/model.ckpt.\n",
            "loss = 373.88907, step = 1\n",
            "global_step/sec: 0.90124\n",
            "loss = 47.537987, step = 101 (110.960 sec)\n",
            "Saving checkpoints for 106 into results/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-12-04-09:17:15\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-106\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Finished evaluation at 2018-12-04-09:17:18\n",
            "Saving dict for global step 106: acc = 0.8873469, acc_ema = 0.8253061, f1 = 0.13218391, f1_ema = 0.025157232, global_step = 106, loss = 106.509056, pr = 0.37704918, pr_ema = 0.031578947, rc = 0.080139376, rc_ema = 0.020905923\n",
            "Saving 'checkpoint_path' summary for global step 106: results/model/model.ckpt-106\n",
            "Saving checkpoints for 184 into results/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-12-04-09:19:15\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-184\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Finished evaluation at 2018-12-04-09:19:18\n",
            "Saving dict for global step 184: acc = 0.8955102, acc_ema = 0.88081634, f1 = 0.24671917, f1_ema = 0.0, global_step = 184, loss = 97.71489, pr = 0.5, pr_ema = 0.0, rc = 0.16376306, rc_ema = 0.0\n",
            "Saving 'checkpoint_path' summary for global step 184: results/model/model.ckpt-184\n",
            "global_step/sec: 0.68173\n",
            "loss = 33.62378, step = 201 (146.686 sec)\n",
            "global_step/sec: 1.23815\n",
            "loss = 36.843678, step = 301 (80.769 sec)\n",
            "Saving checkpoints for 325 into results/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-12-04-09:21:16\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-325\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Finished evaluation at 2018-12-04-09:21:18\n",
            "Saving dict for global step 325: acc = 0.897551, acc_ema = 0.88285714, f1 = 0.2952381, f1_ema = 0.0, global_step = 325, loss = 91.97453, pr = 0.46616542, pr_ema = 0.0, rc = 0.21602787, rc_ema = 0.0\n",
            "Saving 'checkpoint_path' summary for global step 325: results/model/model.ckpt-325\n",
            "global_step/sec: 1.1933\n",
            "loss = 22.444921, step = 401 (83.801 sec)\n",
            "Saving checkpoints for 455 into results/model/model.ckpt.\n",
            "Skip the current checkpoint eval due to throttle secs (120 secs).\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-12-04-09:22:59\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Finished evaluation at 2018-12-04-09:23:02\n",
            "Saving dict for global step 455: acc = 0.9012245, acc_ema = 0.88285714, f1 = 0.45126355, f1_ema = 0.0, global_step = 455, loss = 90.07383, pr = 0.4681648, pr_ema = 0.0, rc = 0.43554008, rc_ema = 0.0\n",
            "Saving 'checkpoint_path' summary for global step 455: results/model/model.ckpt-455\n",
            "Loss for final step: 25.85968.\n",
            "OH\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results/model/model.ckpt-455\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qE_aYO-jcKkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r results /content/drive/My\\ Drive/NLP\\ Project/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkTaUfMwmsRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa6b34fd-5531-4876-8fdc-42cb3ba52f99"
      },
      "cell_type": "code",
      "source": [
        "ls results/score"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testa.tags_ema.preds.txt  testb.tags_ema.preds.txt  train.tags_ema.preds.txt\n",
            "testa.tags.preds.txt      testb.tags.preds.txt      train.tags.preds.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}